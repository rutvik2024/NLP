# -*- coding: utf-8 -*-
"""m22cs011_m20aie263_m22cs059_t2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gd1BN1bWJUFDCA2KTiyQRw5Ys9RFWxLk

# import Required Library
"""

import gzip
import shutil
import os
import pandas as pd
import json
import matplotlib.pyplot as plt
import seaborn as sns
import string
import re
from time import time
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import roc_auc_score, roc_curve
import nltk
from IPython.display import display # Allows the use of display() for DataFrames
from sklearn.feature_extraction.text import CountVectorizer
#tokenize text with Tfidf
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.stem.snowball import SnowballStemmer

#create a stemmer
stemmer = SnowballStemmer("english")

nltk.download('stopwords')

from nltk.corpus import stopwords
stops = set(stopwords.words("english"))

"""# Classifier"""

from sklearn.naive_bayes import MultinomialNB, GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix

# Define classifiers
classifiers = {
    "Naive Bayes (Multinomial)": MultinomialNB(),
    "Naive Bayes (Gaussian)": GaussianNB(),
    "Decision Tree (entropy)": DecisionTreeClassifier(criterion='entropy'),
    "Decision Tree (gini)": DecisionTreeClassifier(criterion='gini'),
    "Random Forest (20 trees)": RandomForestClassifier(n_estimators=20),
    "Random Forest (50 trees)": RandomForestClassifier(n_estimators=50),
    "Random Forest (100 trees)": RandomForestClassifier(n_estimators=100)
}

"""# Extract zip file"""

# Path to the .gz file
input_gz_file = '/content/drive/MyDrive/Dataset/Luxury_Beauty_5.json.gz'

# Path to the output folder where you want to extract the file
output_folder = '/content/drive/MyDrive/Dataset/'

# Open the .gz file for reading and extract it to a temporary file
with gzip.open(input_gz_file, 'rb') as f_in:
    # Construct the path for the temporary file
    # This will append the name of the original file without the .gz extension to the output folder
    # For example, if the input file is 'example.gz', the temporary file will be 'path/to/output/folder/example'
    output_file = output_folder + input_gz_file.split('/')[-1].split('.gz')[0]

    # Open the temporary file for writing
    with open(output_file, 'wb') as f_out:
        # Copy the contents of the .gz file to the temporary file
        shutil.copyfileobj(f_in, f_out)

# Print the path of the extracted file
print("Extracted file:", output_file)

root_dir = '/content/drive/MyDrive/Dataset'
file_name = 'Luxury_Beauty_5.json'

file_path = os.path.join(root_dir, file_name)
print(file_path)

"""# Convert json to csv"""

# Path to the JSON file
input_file_path = file_path

# Path to the output CSV file
output_file_path = os.path.join(root_dir,'Luxury_Beauty_5.csv')

data = []
# Read the JSON file
with open(input_file_path, 'r', encoding='utf-8') as f:
    for line in f:
        data.append(json.loads(line))

# Convert JSON data to pandas DataFrame
df = pd.DataFrame(data)

# Write DataFrame to CSV
df.to_csv(output_file_path, index=False)

print(f"JSON file converted to CSV and saved to {output_file_path}.")

csv_file_path = os.path.join(root_dir, 'Luxury_Beauty_5.csv')
# A size for figures
FIG_SIZE = (14,8)

#Random state for classifiers
RAN_STATE = 42

df = pd.read_csv(csv_file_path)

print("Actual Dataset Size:\nNumber of rows: ",df.shape[0], "\nNumber of columns: ", df.shape[1])
print(df.head(10))

"""# random sample of 20k"""

# Number of entries to sample
sample_size = 20000

# Create a random sample from the DataFrame
random_sample = df.sample(n=sample_size, random_state=42)  # You can change the random_state if you want different samples

# Now random_sample contains the randomly selected 20k entries from the DataFrame
print(random_sample)

print("Random Sample size: \nNumber of rows: ",random_sample.shape[0], "\nNumber of columns: ", random_sample.shape[1])

# Column name for which you want to find unique values
column_name = 'overall'

# Read the CSV file into a DataFrame
df = pd.read_csv(csv_file_path)

# Get unique values in the specified column
unique_values = df[column_name].unique()

# Print unique values
print("Unique values in column '{}':".format(column_name))
for value in unique_values:
    print(value)

### Box Plot
plt.figure(figsize=FIG_SIZE)
plt.title('Box plot of Features')
plt.ylabel('Spread')
plt.xlabel('Features')

display(sns.boxplot(df[df.columns]))

"""# Target Columns selection"""

random_sample['overall'] = random_sample['overall'].astype(object) # fix datatype error
random_sample['reviewText'] = random_sample['reviewText'].astype(object) # fix datatype error

dataset = {"reviewText": random_sample["reviewText"], "overall": random_sample["overall"]  }
print(dataset)

def preprocess_text(text):
    # Convert text to lowercase
    text = text.lower()

    # Remove punctuation using regular expressions
    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)

    return text

def dataset_prep(dataset):
    dataset.loc[:, 'reviewText'] = dataset['reviewText'].str.lower()
    dataset['reviewText']=dataset['reviewText'].apply( lambda x: preprocess_text(x))
    return dataset

"""# Multiclass classification Dataset"""

mul_dataset = pd.DataFrame(data = dataset)
mul_dataset = mul_dataset.dropna()
print(mul_dataset)

mul_dataset = dataset_prep(mul_dataset)
mul_dataset['reviewText'].head(4)

#define our own tokenizing function that we will pass into the TFIDFVectorizer. We will also stem the words here.
def tokens(x):
    x = x.split()
    stems = []
    [stems.append(stemmer.stem(word)) for word in x]
    return stems

def vec_data(dataset):
    #define the vectorizer
    vectorizer = TfidfVectorizer(tokenizer = tokens, stop_words = 'english', ngram_range=(1, 1), min_df = 0.01)
    #fit the vectorizers to the data.
    features = vectorizer.fit_transform(dataset['reviewText'])

    return features

mul_features = vec_data(mul_dataset)

X = mul_features
y = pd.DataFrame(mul_dataset, columns = ["overall"])
# print(X)
train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.1, random_state=RAN_STATE)

print(train_X.shape, test_X.shape)

print(train_y.shape, test_y.shape)

# Train and evaluate classifiers
results = {}
for name, clf in classifiers.items():
    print("Classifier: ",name)
    train_vector = train_X.toarray()
    test_vector = test_X.toarray()

    train_y = train_y.astype('int')
    test_y = test_y.astype('int')

    clf.fit(train_vector, train_y)

    y_pred = clf.predict(test_vector)

    accuracy = accuracy_score(test_y, y_pred)
    confusion_mat = confusion_matrix(test_y, y_pred)
    results[name] = {"accuracy": accuracy, "confusion_matrix": confusion_mat}

# Print results
for name, res in results.items():
    print(f"{name}:")
    print(f"Accuracy: {res['accuracy']}")
    print("Confusion Matrix:")
    print(res['confusion_matrix'])
    print()

"""# Binary Classification"""

bi_dataset = pd.DataFrame(data = dataset)
bi_dataset = bi_dataset.dropna()

bi_dataset["label"] = bi_dataset["overall"].apply(lambda rating : +1 if str(rating) >= '3' else -1)
print(bi_dataset)

bi_features = vec_data(bi_dataset)

X = bi_features
y = pd.DataFrame(bi_dataset, columns = ["label"])
# print(X)
train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.1, random_state=RAN_STATE)

print(train_X.shape, test_X.shape)

print(train_y.shape, test_y.shape)

# Train and evaluate classifiers
bi_results = {}
for name, clf in classifiers.items():
    print("Classifier: ",name)
    train_vector = train_X.toarray()
    test_vector = test_X.toarray()

    train_y = train_y.astype('int')
    test_y = test_y.astype('int')

    clf.fit(train_vector, train_y)

    y_pred = clf.predict(test_vector)

    accuracy = accuracy_score(test_y, y_pred)
    confusion_mat = confusion_matrix(test_y, y_pred)
    results[name] = {"accuracy": accuracy, "confusion_matrix": confusion_mat}

# Print results
for name, res in results.items():
    print(f"{name}:")
    print(f"Accuracy: {res['accuracy']}")
    print("Confusion Matrix:")
    print(res['confusion_matrix'])
    print()

